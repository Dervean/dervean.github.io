---
layout: post
title: "大数据系统: 初识 Hadoop"
author: Dervean
description: "初识 Hadoop"
categories: [hadoop]
tags: [hadoop,linux]
redirect_from:
  - /2018/03/30/
---

大数据系统与大规模数据处理作业，要求环境:

- jdk-1.7
- ubuntu-14.04
- hadoop-2.6.0
- HBase-0.98

---

* Kramdown table of contents
{:toc .toc}

# Hadoop 环境配置

## JDK-1.7

我的 ubuntu 系统版本是 16.04 LTS，jdk 版本是 openjdk-1.8，要求版本是 1.7，需要切换 jdk 版本，但是 16.04 安装源已经没有 openjdk7 了，直接 sudo apt-get install openjdk-7-jdk 会提示 "没有可安装候选"，需要手动添加仓库:

- sudo add-apt-repository ppa:openjdk-r/ppa
- sudo apt-get update
- sudo apt-get install openjdk-7-jdk

然后可以随时切换 jdk 版本:

- sudo update-alternatives --config java
- sudo update-alternatives --config javac

## hadoop-2.6.5

在网上没找到 2.6.0 版本的 hadoop，所以将就着用 2.6.5 版本的，虽然全程按照网上的教程配置的，但也遇到了一些问题，以下是搭建伪分布式环境步骤:

#### 查看 host 文件:

$$
\begin{array}{l}
\text{cat /etc/hosts}
\end{array}
$$

我的 hostname 是 localhost，这个名称待会用于配置文件，因为我搭建的是伪分布式环境，所以 master node 和 slave node 都是 localhost，如果要搭建完全分布式环境，要把每台机子的 host 名称以及 ip 都写入 hosts 文件里面。

#### 免密码连入

免密码连入是为了让 node 之间的数据交流免除输入密码的限制，即使是伪分布式也要做到这一点。

$$
\begin{array}{l}
\text{ssh-keygen -t rsa} \\
\text{cd ~/.ssh} \\
\text{cp id_rsa.pub authorized_keys}
\end{array}
$$

按照网上的教程，到这里应该能够实现免密码 ssh localhost 才对，但是却报错误:

$$
\text{"sign_and_send_pubkey: signing failed: agent refused operation"}
$$

起初我怀疑是因为 authorized_keys 没有生效，所以取消了 /etc/ssh/sshd_config 里面的注释:

$$
\begin{array}{l}
\text{#AuthorizedKeysFile      %h/.ssh/authorized_keys}
\end{array}
$$

并更改权限:

$$
\begin{array}{l}
\text{chmod 700 ~/.ssh} \\
\text{chmod 600 ~/.ssh/authorized_keys}
\end{array}
$$

但仍没有解决上述问题，网上有一个解决方法:

$$
\begin{array}{l}
\text{eval “\$(ssh-agent)”} \\
\text{ssh-add}
\end{array}
$$

ssh-agent 用于管理密钥，ssh-add 将密钥加入到 ssh-agent 中，ssh 和 ssh-agent 通信获取密钥。

可以通过 $\text{ssh-add -l}$ 查看附加了哪些 key。

但这种方法只能在当前的 terminal 起作用，重新开一个 terminal 就得重新执行上述命令。

最终我重打开一个 terminal 直接执行:

$$
\begin{array}{l}
\text{ssh-add}
\end{array}
$$

竟然成功了，当然我也没弄懂其中缘由，可能这就是天意吧 :)

#### 安装 hadoop-2.6.5

下载 hadoop-2.6.5.tar.gz 并解压:

$$
\begin{array}{l}
\text{tar xzvf hadoop-2.6.5.tar.gz ~/Hadoop/hadoop-2.6.5}
\end{array}
$$

然后在 hadoop-2.6.5 目录下修改以下文件:

##### $\text{./etc/hadoop/hadoop-env.sh}$

~~~xml
   export JAVA_HOME= /usr/lib/jvm/java-7-openjdk-amd64
~~~

##### $\text{./etc/hadoop/core-site.xml}$

~~~xml
   <configuration>
		<property>
			<name>fs.default.name</name>
			<value>hdfs://localhost:9000</value>
			<final>true</final>
		</property>

		<property>
			<name>hadoop.tmp.dir</name>
			<value>/home/dervean/Hadoop/hadoop-2.6.5/tmp</value>
			<description>A base for other temporary directories</description>
		</property>
	</configuration>
~~~

1.设置 master 的 NameNode 的 IP 以及监听端口

2.设置存放每次运行的作业信息的临时目录。

##### $\text{./etc/hadoop/hdfs-site.xml}$

~~~xml
	<configuration>
		<property>
			<name>dfs.name.dir</name>
			<value>file:/home/dervean/Hadoop/hadoop-2.6.5/name</value>
			<final>true</final>
		</property>
	
		<property>
			<name>dfs.data.dir</name>
			<value>file:/home/dervean/Hadoop/hadoop-2.6.5/data</value>
			<final>true</final>
		</property>
	
		<property>
			<name>dfs.replication</name>
			<value>1</value>
			<final>true</final>
		</property>
	
		<property>
			<name>dfs.permissions</name>
			<value>false</value>
		</property>
	</configuration>
~~~

1.设置 NameNode 存储元数据的目录；

2.设置 DataNode 存放数据块的目录；

3.设置副本数目；设置访问权限。

注意必须使用规范的 URI 格式（即: file:/home/dervean/Hadoop/hadoop-2.6.5/data 不要缺失 "file:"）。

##### $\text{./etc/hadoop/mapred-site.xml}$

~~~xml
	<configuration>
		<property>
			<name>mapred.job.tracker</name>
			<value>localhost:9001</value>
		</property>
	</configuration>
~~~

设置 JobTracker 的 IP 以及监听端口。

##### $\text{./etc/hadoop/slaves}$

~~~xml
	localhost
~~~
   
因为是伪分布式，所以 slave 只有 localhost。

也可以修改 /etc/profile 文件将 hadoop 命令全部加入环境变量里面，我没有做这一步。

#### 尝试启动集群:

$$
\begin{array}{l}
\text{./bin/hadoop namenode -format}
\end{array}
$$

格式化操作**只需要一次**即可。

$$
\begin{array}{l}
\text{./sbin/start-all.sh}
\end{array}
$$

查看启动情况:

$$
\begin{array}{l}
\text{jps}
\end{array}
$$

如果出现 NameNode、NodeManager、ResourceManager、SecondaryNameNode、DataNode 这五个进程则说明启动成功。

遇到一些问题的话可以从 hadoop-2.6.5 目录下的 logs 目录中查找相关内容。

## HBase-0.98















